{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import math\n",
    "from matplotlib import patches\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from robopoint.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from robopoint.conversation import conv_templates\n",
    "from robopoint.model.builder import load_pretrained_model\n",
    "from robopoint.utils import disable_torch_init\n",
    "from robopoint.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path\n",
    "disable_torch_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robopoint.minimal.extract_feat import extract_mm_proj\n",
    "\n",
    "image_path = \"/robodata/smodak/repos/f3rm/f3rm/scripts/images/frame_1.png\"\n",
    "descriptors = extract_mm_proj(image_paths=[image_path], device=\"cuda\")\n",
    "descriptors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\"chairs.png\"]\n",
    "device = \"cuda\"\n",
    "# model_path = \"wentao-yuan/robopoint-v1-vicuna-v1.5-13b\"\n",
    "model_path = \"liuhaotian/llava-v1.5-13b\"\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path)\n",
    ")\n",
    "\n",
    "vision_tower = model.get_model().get_vision_tower()\n",
    "mm_projector = model.get_model().mm_projector\n",
    "\n",
    "# Preprocess the images\n",
    "images = [Image.open(path).convert('RGB') for path in image_paths]\n",
    "preprocessed_images = process_images(images, image_processor, model.config)\n",
    "preprocessed_images = preprocessed_images.to(device)  # (b, 3, h, w)\n",
    "print(f\"Preprocessed {len(images)} images into {preprocessed_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ea7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = preprocessed_images[0: 64]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bcd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feat = vision_tower(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d95dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_projector(img_feat).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"wentao-yuan/robopoint-v1-vicuna-v1.5-13b\"\n",
    "image_path = \"between.png\"\n",
    "# image_path = \"/robodata/smodak/repos/f3rm/f3rm/scripts/images/frame_1.png\"\n",
    "question = \"Identify locations on the floor in the vacant space between the two chairs. Your answer should be formatted as a list of tuples, i.e. [(x1, y1), (x2, y2), ...], where each tuple contains the x and y coordinates of a point satisfying the conditions above. The coordinates should be between 0 and 1, indicating the normalized pixel locations of the points in the image. Return empty list if no such points exist.\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ddd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.vision_tower.vision_tower.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda11a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.vision_tower.select_feature, model.model.vision_tower.select_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab882d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701da90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.mm_projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c85cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor.do_rescale = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06635bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_model().get_vision_tower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d31514",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = process_images([image], image_processor, model.config)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = image_tensor.unsqueeze(0).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c0745",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b348a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = model.model.vision_tower.vision_tower(images, output_hidden_states=True)\n",
    "ii.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c460aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = model.get_model().get_vision_tower()(images)\n",
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features2 = model.get_model().mm_projector(image_features)\n",
    "image_features2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c54cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.vision_tower.select_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_model().mm_projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33086bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad1dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "model = AutoModelForZeroShotImageClassification.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef62fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d30d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "\n",
    "@torch.inference_mode()\n",
    "def preprocess(image_path, load_size, patch_size, mean, std, allow_crop=False):\n",
    "    \"\"\"\n",
    "    Preprocesses an image before extraction.\n",
    "    :param image_path: path to image to be extracted, or a PIL image.\n",
    "    :param load_size: optional. Size to resize image before the rest of preprocessing. -1 to use smallest side size.\n",
    "    :param allow_crop: optional. If True, crop the image to be divisible by the patch size.\n",
    "    :return: a tuple containing:\n",
    "                (1) the preprocessed image as a tensor to insert the model of shape BxCxHxW.\n",
    "                (2) the pil image in relevant dimensions\n",
    "    \"\"\"\n",
    "    if isinstance(image_path, str) or isinstance(image_path, Path):\n",
    "        pil_image = Image.open(image_path).convert('RGB')\n",
    "    elif isinstance(image_path, Image.Image):\n",
    "        pil_image = image_path.convert('RGB')\n",
    "    pil_image = transforms.ToTensor()(pil_image)\n",
    "    if allow_crop:\n",
    "        height, width = pil_image.shape[1:]   # C x H x W\n",
    "        cropped_width, cropped_height = width - width % patch_size, height - height % patch_size\n",
    "        pil_image = pil_image[:, :cropped_height, :cropped_width]\n",
    "    else:\n",
    "        cropped_width, cropped_height = pil_image.shape[2], pil_image.shape[1]\n",
    "    if load_size is not None:\n",
    "        if load_size == -1:\n",
    "            load_size = min(pil_image.shape[1:])\n",
    "        pil_image = transforms.Resize(load_size, interpolation=transforms.InterpolationMode.BICUBIC)(pil_image)\n",
    "    prep = transforms.Compose([\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    prep_img = prep(pil_image)\n",
    "    prep_img = prep_img[None, ...]\n",
    "    return prep_img, pil_image, cropped_height, cropped_width\n",
    "\n",
    "\n",
    "def compute_new_dims(orig_size, short_side=224, round_multiple=14):\n",
    "    \"\"\"\n",
    "    Scales the input (width, height) so that the smaller side becomes 'short_side',\n",
    "    then rounds both dimensions to a multiple of 'round_multiple'.\n",
    "    \"\"\"\n",
    "    w, h = orig_size\n",
    "\n",
    "    # Figure out which side is smaller and compute scale factor\n",
    "    if w <= h:\n",
    "        scale = short_side / w\n",
    "    else:\n",
    "        scale = short_side / h\n",
    "\n",
    "    # Scale both dimensions\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "\n",
    "    # Round each dimension to the nearest multiple of 'round_multiple'\n",
    "    new_w = (new_w + round_multiple - 1) // round_multiple * round_multiple\n",
    "    new_h = (new_h + round_multiple - 1) // round_multiple * round_multiple\n",
    "\n",
    "    return (new_w, new_h)\n",
    "\n",
    "\n",
    "def resize_to_match_aspect_ratio(pil_img, target_ratio_tuple=(1280, 720)):\n",
    "    _img = deepcopy(pil_img)\n",
    "    target_w, target_h = target_ratio_tuple\n",
    "    target_aspect = target_w / target_h\n",
    "    w, h = _img.size\n",
    "    current_aspect = w / h\n",
    "    if abs(current_aspect - target_aspect) < 1e-6:\n",
    "        return _img  # Already matches aspect ratio\n",
    "    if current_aspect > target_aspect:\n",
    "        # Image is too wide → increase height\n",
    "        new_h = round(w / target_aspect)\n",
    "        new_size = (w, new_h)\n",
    "    else:\n",
    "        # Image is too tall → increase width\n",
    "        new_w = round(h * target_aspect)\n",
    "        new_size = (new_w, h)\n",
    "    return _img.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def run_pca(tokens, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(tokens)\n",
    "    projected_tokens = pca.transform(tokens)\n",
    "    return projected_tokens\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def run_pca_2(tokens, n_components=3, proj_V=None, low_rank_min=None, low_rank_max=None, niter=5, q_min=0.01, q_max=0.99):\n",
    "    \"\"\"\n",
    "    Adapted from the distilled-feature-fields code; uses torch.pca_lowrank \n",
    "    and quantile-based clamping for PCA-based projection to 3 channels.\n",
    "    \"\"\"\n",
    "    tokens_t = torch.as_tensor(tokens, dtype=torch.float32)\n",
    "\n",
    "    # 1) Compute PCA basis if not provided\n",
    "    if proj_V is None:\n",
    "        mean = tokens_t.mean(dim=0)\n",
    "        shifted = tokens_t - mean\n",
    "        # Perform low-rank approximation (PCA) in PyTorch\n",
    "        U, S, V = torch.pca_lowrank(shifted, q=n_components, niter=niter)\n",
    "        proj_V = V[:, :n_components]  # top n_components\n",
    "\n",
    "    # 2) Project into 3D\n",
    "    projected_tokens = tokens_t @ proj_V\n",
    "\n",
    "    # 3) Compute quantile-based min/max if not provided\n",
    "    if low_rank_min is None:\n",
    "        low_rank_min = torch.quantile(projected_tokens, q_min, dim=0)\n",
    "    if low_rank_max is None:\n",
    "        low_rank_max = torch.quantile(projected_tokens, q_max, dim=0)\n",
    "\n",
    "    # 4) Scale to [0,1] and clamp\n",
    "    projected_tokens = (projected_tokens - low_rank_min) / (low_rank_max - low_rank_min)\n",
    "    projected_tokens = projected_tokens.clamp(0, 1)\n",
    "\n",
    "    # Return the 3D result plus the projection matrix & min/max for reuse\n",
    "    return projected_tokens.numpy(), proj_V, low_rank_min, low_rank_max\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def viz_pca3(projected_tokens, grid_size, orig_img_width, orig_img_height, resample=Image.LANCZOS) -> Image:\n",
    "    t = torch.tensor(projected_tokens)\n",
    "    t_min = t.min(dim=0, keepdim=True).values\n",
    "    t_max = t.max(dim=0, keepdim=True).values\n",
    "    normalized_t = (t - t_min) / (t_max - t_min)\n",
    "    array = (normalized_t * 255).byte().numpy()\n",
    "    array = array.reshape(*grid_size, 3)\n",
    "    return Image.fromarray(array).resize((orig_img_width, orig_img_height), resample=resample)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def viz_pca3_2(projected_tokens, grid_size, orig_img_width, orig_img_height, resample=Image.LANCZOS) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Adapted from the distilled-feature-fields code; Take a (N x 3) array in [0,1], reshape to the specified grid_size, and map to RGB image.\n",
    "    \"\"\"\n",
    "    # Convert [0,1] -> [0,255], reshape to (H, W, 3)\n",
    "    arr = (projected_tokens * 255).astype(\"uint8\").reshape(*grid_size, 3)\n",
    "    img = Image.fromarray(arr)\n",
    "    return img.resize((orig_img_width, orig_img_height), resample=resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors1 = image_features.cpu().squeeze()\n",
    "projected_tokens1 = run_pca(descriptors1, n_components=3)\n",
    "img1 = viz_pca3(projected_tokens1, (24,24), 336, 336)\n",
    "plt.imshow(img1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeefc7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b65eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965bec89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f3rm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
